Loading data...
(?, 32, 32, 128)
(?, 30, 30, 128)
(?, 15, 15, 128)
(?, 15, 15, 256)
(?, 13, 13, 256)
(?, 6, 6, 256)
(?, 6, 6, 512)
(?, 4, 4, 512)
(?, 2, 2, 512)
Training
Iteration 0: with minibatch training loss = 5.17 and accuracy of 0
Iteration 128: with minibatch training loss = 4.97 and accuracy of 0.0078
Iteration 256: with minibatch training loss = 4.93 and accuracy of 0.023
Iteration 384: with minibatch training loss = 4.72 and accuracy of 0.016
Iteration 512: with minibatch training loss = 4.76 and accuracy of 0.016
Iteration 640: with minibatch training loss = 4.68 and accuracy of 0.039
Epoch 1, Overall loss = 4.85 and accuracy of 0.0146
Iteration 768: with minibatch training loss = 4.48 and accuracy of 0.016
Iteration 896: with minibatch training loss = 4.5 and accuracy of 0.031
Iteration 1024: with minibatch training loss = 4.46 and accuracy of 0.047
Iteration 1152: with minibatch training loss = 4.3 and accuracy of 0.016
Iteration 1280: with minibatch training loss = 4.42 and accuracy of 0.039
Iteration 1408: with minibatch training loss = 4.17 and accuracy of 0.062
Epoch 2, Overall loss = 4.41 and accuracy of 0.0343
Iteration 1536: with minibatch training loss = 4.19 and accuracy of 0.047
Iteration 1664: with minibatch training loss = 4.19 and accuracy of 0.047
Iteration 1792: with minibatch training loss = 4.23 and accuracy of 0.023
Iteration 1920: with minibatch training loss = 4.21 and accuracy of 0.047
Iteration 2048: with minibatch training loss = 4.15 and accuracy of 0.094
Iteration 2176: with minibatch training loss = 4.14 and accuracy of 0.039
Epoch 3, Overall loss = 4.23 and accuracy of 0.0524
Iteration 2304: with minibatch training loss = 4.18 and accuracy of 0.078
Iteration 2432: with minibatch training loss = 4.12 and accuracy of 0.062
Iteration 2560: with minibatch training loss = 4.02 and accuracy of 0.062
Iteration 2688: with minibatch training loss = 4.12 and accuracy of 0.078
Iteration 2816: with minibatch training loss = 4.05 and accuracy of 0.039
Iteration 2944: with minibatch training loss = 4.05 and accuracy of 0.086
Epoch 4, Overall loss = 4.1 and accuracy of 0.0716
Iteration 3072: with minibatch training loss = 4.1 and accuracy of 0.07
Iteration 3200: with minibatch training loss = 3.89 and accuracy of 0.1
Iteration 3328: with minibatch training loss = 3.9 and accuracy of 0.1
Iteration 3456: with minibatch training loss = 3.91 and accuracy of 0.086
Iteration 3584: with minibatch training loss = 3.99 and accuracy of 0.13
Iteration 3712: with minibatch training loss = 3.98 and accuracy of 0.11
Epoch 5, Overall loss = 3.99 and accuracy of 0.0884
Iteration 3840: with minibatch training loss = 4.01 and accuracy of 0.11
Iteration 3968: with minibatch training loss = 3.88 and accuracy of 0.12
Iteration 4096: with minibatch training loss = 3.78 and accuracy of 0.13
Iteration 4224: with minibatch training loss = 3.84 and accuracy of 0.086
Iteration 4352: with minibatch training loss = 3.92 and accuracy of 0.1
Epoch 6, Overall loss = 3.91 and accuracy of 0.102
Iteration 4480: with minibatch training loss = 3.86 and accuracy of 0.094
Iteration 4608: with minibatch training loss = 3.87 and accuracy of 0.12
Iteration 4736: with minibatch training loss = 3.68 and accuracy of 0.16
Iteration 4864: with minibatch training loss = 3.84 and accuracy of 0.086
Iteration 4992: with minibatch training loss = 3.91 and accuracy of 0.1
Iteration 5120: with minibatch training loss = 3.86 and accuracy of 0.094
Epoch 7, Overall loss = 3.83 and accuracy of 0.117
Iteration 5248: with minibatch training loss = 3.93 and accuracy of 0.1
Iteration 5376: with minibatch training loss = 3.67 and accuracy of 0.15
Iteration 5504: with minibatch training loss = 3.86 and accuracy of 0.16
Iteration 5632: with minibatch training loss = 3.72 and accuracy of 0.11
Iteration 5760: with minibatch training loss = 3.71 and accuracy of 0.14
Iteration 5888: with minibatch training loss = 3.56 and accuracy of 0.16
Epoch 8, Overall loss = 3.75 and accuracy of 0.129
Iteration 6016: with minibatch training loss = 3.76 and accuracy of 0.12
Iteration 6144: with minibatch training loss = 3.68 and accuracy of 0.14
Iteration 6272: with minibatch training loss = 3.7 and accuracy of 0.16
Iteration 6400: with minibatch training loss = 3.91 and accuracy of 0.16
Iteration 6528: with minibatch training loss = 3.73 and accuracy of 0.12
Iteration 6656: with minibatch training loss = 3.76 and accuracy of 0.1
Epoch 9, Overall loss = 3.68 and accuracy of 0.141
Iteration 6784: with minibatch training loss = 3.88 and accuracy of 0.11
Iteration 6912: with minibatch training loss = 3.49 and accuracy of 0.15
Iteration 7040: with minibatch training loss = 3.63 and accuracy of 0.13
Iteration 7168: with minibatch training loss = 3.63 and accuracy of 0.15
Iteration 7296: with minibatch training loss = 3.67 and accuracy of 0.16
Iteration 7424: with minibatch training loss = 3.5 and accuracy of 0.17
Epoch 10, Overall loss = 3.6 and accuracy of 0.156
Iteration 7552: with minibatch training loss = 3.49 and accuracy of 0.18
Iteration 7680: with minibatch training loss = 3.48 and accuracy of 0.24
Iteration 7808: with minibatch training loss = 3.46 and accuracy of 0.16
Iteration 7936: with minibatch training loss = 3.27 and accuracy of 0.28
Iteration 8064: with minibatch training loss = 3.48 and accuracy of 0.19
Epoch 11, Overall loss = 3.54 and accuracy of 0.168
Iteration 8192: with minibatch training loss = 3.62 and accuracy of 0.2
Iteration 8320: with minibatch training loss = 3.48 and accuracy of 0.18
Iteration 8448: with minibatch training loss = 3.61 and accuracy of 0.14
Iteration 8576: with minibatch training loss = 3.38 and accuracy of 0.19
Iteration 8704: with minibatch training loss = 3.51 and accuracy of 0.16
Iteration 8832: with minibatch training loss = 3.8 and accuracy of 0.13
Epoch 12, Overall loss = 3.47 and accuracy of 0.179
Iteration 8960: with minibatch training loss = 3.54 and accuracy of 0.15
Iteration 9088: with minibatch training loss = 3.38 and accuracy of 0.17
Iteration 9216: with minibatch training loss = 3.34 and accuracy of 0.2
Iteration 9344: with minibatch training loss = 3.36 and accuracy of 0.23
Iteration 9472: with minibatch training loss = 3.47 and accuracy of 0.15
Iteration 9600: with minibatch training loss = 3.28 and accuracy of 0.2
Epoch 13, Overall loss = 3.4 and accuracy of 0.192
Iteration 9728: with minibatch training loss = 3.29 and accuracy of 0.2
Iteration 9856: with minibatch training loss = 3.42 and accuracy of 0.13
Iteration 9984: with minibatch training loss = 3.46 and accuracy of 0.18
Iteration 10112: with minibatch training loss = 3.22 and accuracy of 0.22
Iteration 10240: with minibatch training loss = 3.26 and accuracy of 0.23
Iteration 10368: with minibatch training loss = 3.05 and accuracy of 0.29
Epoch 14, Overall loss = 3.34 and accuracy of 0.204
Iteration 10496: with minibatch training loss = 3.25 and accuracy of 0.2
Iteration 10624: with minibatch training loss = 3.3 and accuracy of 0.18
Iteration 10752: with minibatch training loss = 3.28 and accuracy of 0.18
Iteration 10880: with minibatch training loss = 3.24 and accuracy of 0.24
Iteration 11008: with minibatch training loss = 3.2 and accuracy of 0.18
Iteration 11136: with minibatch training loss = 3.09 and accuracy of 0.27
Epoch 15, Overall loss = 3.28 and accuracy of 0.212
Iteration 11264: with minibatch training loss = 3.15 and accuracy of 0.27
Iteration 11392: with minibatch training loss = 3.13 and accuracy of 0.27
Iteration 11520: with minibatch training loss = 3.45 and accuracy of 0.16
Iteration 11648: with minibatch training loss = 3.37 and accuracy of 0.23
Iteration 11776: with minibatch training loss = 3.11 and accuracy of 0.27
Epoch 16, Overall loss = 3.22 and accuracy of 0.223
Iteration 11904: with minibatch training loss = 3.09 and accuracy of 0.23
Iteration 12032: with minibatch training loss = 3.07 and accuracy of 0.26
Iteration 12160: with minibatch training loss = 3.06 and accuracy of 0.24
Iteration 12288: with minibatch training loss = 3.33 and accuracy of 0.16
Iteration 12416: with minibatch training loss = 3.04 and accuracy of 0.28
Iteration 12544: with minibatch training loss = 3.27 and accuracy of 0.21
Epoch 17, Overall loss = 3.17 and accuracy of 0.232
Iteration 12672: with minibatch training loss = 3.16 and accuracy of 0.24
Iteration 12800: with minibatch training loss = 3.1 and accuracy of 0.23
Iteration 12928: with minibatch training loss = 3.05 and accuracy of 0.22
Iteration 13056: with minibatch training loss = 3.12 and accuracy of 0.23
Iteration 13184: with minibatch training loss = 3.12 and accuracy of 0.26
Iteration 13312: with minibatch training loss = 2.85 and accuracy of 0.26
Epoch 18, Overall loss = 3.12 and accuracy of 0.242
Iteration 13440: with minibatch training loss = 3.18 and accuracy of 0.18
Iteration 13568: with minibatch training loss = 3.21 and accuracy of 0.19
Iteration 13696: with minibatch training loss = 2.95 and accuracy of 0.27
Iteration 13824: with minibatch training loss = 3.02 and accuracy of 0.24
Iteration 13952: with minibatch training loss = 2.91 and accuracy of 0.26
Iteration 14080: with minibatch training loss = 2.76 and accuracy of 0.37
Epoch 19, Overall loss = 3.07 and accuracy of 0.249
Iteration 14208: with minibatch training loss = 3.13 and accuracy of 0.24
Iteration 14336: with minibatch training loss = 3.14 and accuracy of 0.21
Iteration 14464: with minibatch training loss = 3.16 and accuracy of 0.22
Iteration 14592: with minibatch training loss = 2.95 and accuracy of 0.34
Iteration 14720: with minibatch training loss = 3.14 and accuracy of 0.23
Iteration 14848: with minibatch training loss = 2.9 and accuracy of 0.28
Epoch 20, Overall loss = 3.02 and accuracy of 0.258
Iteration 14976: with minibatch training loss = 3 and accuracy of 0.26
Iteration 15104: with minibatch training loss = 3.06 and accuracy of 0.25
Iteration 15232: with minibatch training loss = 2.82 and accuracy of 0.34
Iteration 15360: with minibatch training loss = 2.72 and accuracy of 0.31
Iteration 15488: with minibatch training loss = 2.83 and accuracy of 0.31
Epoch 21, Overall loss = 2.98 and accuracy of 0.268
Iteration 15616: with minibatch training loss = 2.98 and accuracy of 0.34
Iteration 15744: with minibatch training loss = 2.82 and accuracy of 0.29
Iteration 15872: with minibatch training loss = 2.78 and accuracy of 0.3
Iteration 16000: with minibatch training loss = 2.89 and accuracy of 0.27
Iteration 16128: with minibatch training loss = 3.01 and accuracy of 0.27
Iteration 16256: with minibatch training loss = 3.02 and accuracy of 0.2
Epoch 22, Overall loss = 2.94 and accuracy of 0.274
Iteration 16384: with minibatch training loss = 3.01 and accuracy of 0.24
Iteration 16512: with minibatch training loss = 3 and accuracy of 0.27
Iteration 16640: with minibatch training loss = 2.95 and accuracy of 0.31
Iteration 16768: with minibatch training loss = 2.85 and accuracy of 0.26
Iteration 16896: with minibatch training loss = 2.89 and accuracy of 0.3
Iteration 17024: with minibatch training loss = 3.03 and accuracy of 0.28
Epoch 23, Overall loss = 2.9 and accuracy of 0.282
Iteration 17152: with minibatch training loss = 2.9 and accuracy of 0.23
Iteration 17280: with minibatch training loss = 2.55 and accuracy of 0.35
Iteration 17408: with minibatch training loss = 2.93 and accuracy of 0.3
Iteration 17536: with minibatch training loss = 2.81 and accuracy of 0.29
Iteration 17664: with minibatch training loss = 2.78 and accuracy of 0.3
Iteration 17792: with minibatch training loss = 2.73 and accuracy of 0.3
Epoch 24, Overall loss = 2.85 and accuracy of 0.29
Iteration 17920: with minibatch training loss = 2.86 and accuracy of 0.3
Iteration 18048: with minibatch training loss = 2.9 and accuracy of 0.28
Iteration 18176: with minibatch training loss = 2.84 and accuracy of 0.29
Iteration 18304: with minibatch training loss = 2.74 and accuracy of 0.3
Iteration 18432: with minibatch training loss = 2.51 and accuracy of 0.39
Iteration 18560: with minibatch training loss = 2.97 and accuracy of 0.29
Epoch 25, Overall loss = 2.81 and accuracy of 0.298
Iteration 18688: with minibatch training loss = 2.95 and accuracy of 0.24
Iteration 18816: with minibatch training loss = 3.1 and accuracy of 0.27
Iteration 18944: with minibatch training loss = 2.72 and accuracy of 0.31
Iteration 19072: with minibatch training loss = 2.64 and accuracy of 0.36
Iteration 19200: with minibatch training loss = 3.02 and accuracy of 0.24
Epoch 26, Overall loss = 2.78 and accuracy of 0.304
Iteration 19328: with minibatch training loss = 2.63 and accuracy of 0.34
Iteration 19456: with minibatch training loss = 2.85 and accuracy of 0.25
Iteration 19584: with minibatch training loss = 2.95 and accuracy of 0.22
Iteration 19712: with minibatch training loss = 2.69 and accuracy of 0.31
Iteration 19840: with minibatch training loss = 2.71 and accuracy of 0.34
Iteration 19968: with minibatch training loss = 2.73 and accuracy of 0.26
Epoch 27, Overall loss = 2.74 and accuracy of 0.312
Iteration 20096: with minibatch training loss = 2.73 and accuracy of 0.39
Iteration 20224: with minibatch training loss = 2.77 and accuracy of 0.3
Iteration 20352: with minibatch training loss = 2.61 and accuracy of 0.37
Iteration 20480: with minibatch training loss = 2.53 and accuracy of 0.34
Iteration 20608: with minibatch training loss = 2.67 and accuracy of 0.33
Iteration 20736: with minibatch training loss = 2.52 and accuracy of 0.38
Epoch 28, Overall loss = 2.71 and accuracy of 0.318
Iteration 20864: with minibatch training loss = 2.85 and accuracy of 0.3
Iteration 20992: with minibatch training loss = 2.44 and accuracy of 0.42
Iteration 21120: with minibatch training loss = 2.75 and accuracy of 0.3
Iteration 21248: with minibatch training loss = 2.69 and accuracy of 0.35
Iteration 21376: with minibatch training loss = 2.5 and accuracy of 0.31
Iteration 21504: with minibatch training loss = 2.55 and accuracy of 0.39
Epoch 29, Overall loss = 2.68 and accuracy of 0.323
Iteration 21632: with minibatch training loss = 2.64 and accuracy of 0.3
Iteration 21760: with minibatch training loss = 2.52 and accuracy of 0.38
Iteration 21888: with minibatch training loss = 2.43 and accuracy of 0.39
Iteration 22016: with minibatch training loss = 2.46 and accuracy of 0.35
Iteration 22144: with minibatch training loss = 2.71 and accuracy of 0.36
Iteration 22272: with minibatch training loss = 2.76 and accuracy of 0.28
Epoch 30, Overall loss = 2.64 and accuracy of 0.331
Iteration 22400: with minibatch training loss = 2.67 and accuracy of 0.32
Iteration 22528: with minibatch training loss = 2.78 and accuracy of 0.32
Iteration 22656: with minibatch training loss = 2.54 and accuracy of 0.38
Iteration 22784: with minibatch training loss = 2.68 and accuracy of 0.27
Iteration 22912: with minibatch training loss = 2.41 and accuracy of 0.39
Epoch 31, Overall loss = 2.6 and accuracy of 0.337
Iteration 23040: with minibatch training loss = 2.69 and accuracy of 0.29
Iteration 23168: with minibatch training loss = 2.62 and accuracy of 0.32
Iteration 23296: with minibatch training loss = 2.76 and accuracy of 0.27
Iteration 23424: with minibatch training loss = 2.83 and accuracy of 0.34
Iteration 23552: with minibatch training loss = 2.53 and accuracy of 0.33
Iteration 23680: with minibatch training loss = 2.59 and accuracy of 0.35
Epoch 32, Overall loss = 2.58 and accuracy of 0.344
Iteration 23808: with minibatch training loss = 2.67 and accuracy of 0.34
Iteration 23936: with minibatch training loss = 2.68 and accuracy of 0.33
Iteration 24064: with minibatch training loss = 2.7 and accuracy of 0.28
Iteration 24192: with minibatch training loss = 2.48 and accuracy of 0.34
Iteration 24320: with minibatch training loss = 2.55 and accuracy of 0.35
Iteration 24448: with minibatch training loss = 2.49 and accuracy of 0.37
Epoch 33, Overall loss = 2.55 and accuracy of 0.347
Iteration 24576: with minibatch training loss = 2.47 and accuracy of 0.38
Iteration 24704: with minibatch training loss = 2.46 and accuracy of 0.36
Iteration 24832: with minibatch training loss = 2.67 and accuracy of 0.37
Iteration 24960: with minibatch training loss = 2.37 and accuracy of 0.43
Iteration 25088: with minibatch training loss = 2.45 and accuracy of 0.37
Iteration 25216: with minibatch training loss = 2.77 and accuracy of 0.3
Epoch 34, Overall loss = 2.51 and accuracy of 0.356
Iteration 25344: with minibatch training loss = 2.57 and accuracy of 0.38
Iteration 25472: with minibatch training loss = 2.56 and accuracy of 0.3
Iteration 25600: with minibatch training loss = 2.6 and accuracy of 0.31
Iteration 25728: with minibatch training loss = 2.46 and accuracy of 0.34
Iteration 25856: with minibatch training loss = 2.53 and accuracy of 0.33
Iteration 25984: with minibatch training loss = 2.4 and accuracy of 0.35
Epoch 35, Overall loss = 2.49 and accuracy of 0.361
Iteration 26112: with minibatch training loss = 2.4 and accuracy of 0.38
Iteration 26240: with minibatch training loss = 2.6 and accuracy of 0.31
Iteration 26368: with minibatch training loss = 2.31 and accuracy of 0.4
Iteration 26496: with minibatch training loss = 2.41 and accuracy of 0.37
Iteration 26624: with minibatch training loss = 2.51 and accuracy of 0.35
Epoch 36, Overall loss = 2.46 and accuracy of 0.365
Iteration 26752: with minibatch training loss = 2.21 and accuracy of 0.44
Iteration 26880: with minibatch training loss = 2.39 and accuracy of 0.32
Iteration 27008: with minibatch training loss = 2.33 and accuracy of 0.38
Iteration 27136: with minibatch training loss = 2.34 and accuracy of 0.4
Iteration 27264: with minibatch training loss = 2.29 and accuracy of 0.36
Iteration 27392: with minibatch training loss = 2.73 and accuracy of 0.35
Epoch 37, Overall loss = 2.42 and accuracy of 0.372
Iteration 27520: with minibatch training loss = 2.31 and accuracy of 0.36
Iteration 27648: with minibatch training loss = 2.48 and accuracy of 0.33
Iteration 27776: with minibatch training loss = 2.49 and accuracy of 0.35
Iteration 27904: with minibatch training loss = 2.67 and accuracy of 0.34
Iteration 28032: with minibatch training loss = 2.29 and accuracy of 0.43
Iteration 28160: with minibatch training loss = 2.4 and accuracy of 0.36
Epoch 38, Overall loss = 2.4 and accuracy of 0.381
Iteration 28288: with minibatch training loss = 2.41 and accuracy of 0.38
Iteration 28416: with minibatch training loss = 2.53 and accuracy of 0.36
Iteration 28544: with minibatch training loss = 2.37 and accuracy of 0.36
Iteration 28672: with minibatch training loss = 2.37 and accuracy of 0.39
Iteration 28800: with minibatch training loss = 2.12 and accuracy of 0.44
Iteration 28928: with minibatch training loss = 2.54 and accuracy of 0.34
Epoch 39, Overall loss = 2.37 and accuracy of 0.386
Iteration 29056: with minibatch training loss = 2.49 and accuracy of 0.36
Iteration 29184: with minibatch training loss = 2.34 and accuracy of 0.38
Iteration 29312: with minibatch training loss = 2.22 and accuracy of 0.44
Iteration 29440: with minibatch training loss = 2.53 and accuracy of 0.34
Iteration 29568: with minibatch training loss = 2.33 and accuracy of 0.37
Iteration 29696: with minibatch training loss = 2.54 and accuracy of 0.27
Epoch 40, Overall loss = 2.34 and accuracy of 0.389
Iteration 29824: with minibatch training loss = 2.38 and accuracy of 0.4
Iteration 29952: with minibatch training loss = 2.5 and accuracy of 0.34
Iteration 30080: with minibatch training loss = 2.23 and accuracy of 0.41
Iteration 30208: with minibatch training loss = 2.29 and accuracy of 0.41
Iteration 30336: with minibatch training loss = 2.24 and accuracy of 0.36
Epoch 41, Overall loss = 2.32 and accuracy of 0.396
Iteration 30464: with minibatch training loss = 2.4 and accuracy of 0.41
Iteration 30592: with minibatch training loss = 2.38 and accuracy of 0.38
Iteration 30720: with minibatch training loss = 2.19 and accuracy of 0.47
Iteration 30848: with minibatch training loss = 2.34 and accuracy of 0.39
Iteration 30976: with minibatch training loss = 2.11 and accuracy of 0.45
Iteration 31104: with minibatch training loss = 2.09 and accuracy of 0.45
Epoch 42, Overall loss = 2.29 and accuracy of 0.401
Iteration 31232: with minibatch training loss = 2.19 and accuracy of 0.44
Iteration 31360: with minibatch training loss = 2.25 and accuracy of 0.35
Iteration 31488: with minibatch training loss = 2.45 and accuracy of 0.35
Iteration 31616: with minibatch training loss = 2.22 and accuracy of 0.38
Iteration 31744: with minibatch training loss = 2.01 and accuracy of 0.45
Iteration 31872: with minibatch training loss = 2.47 and accuracy of 0.36
Epoch 43, Overall loss = 2.27 and accuracy of 0.406
Iteration 32000: with minibatch training loss = 2.26 and accuracy of 0.39
Iteration 32128: with minibatch training loss = 2.08 and accuracy of 0.46
Iteration 32256: with minibatch training loss = 2.09 and accuracy of 0.45
Iteration 32384: with minibatch training loss = 2.25 and accuracy of 0.44
Iteration 32512: with minibatch training loss = 2.47 and accuracy of 0.34
Iteration 32640: with minibatch training loss = 2.6 and accuracy of 0.3
Epoch 44, Overall loss = 2.24 and accuracy of 0.415
Iteration 32768: with minibatch training loss = 2.2 and accuracy of 0.41
Iteration 32896: with minibatch training loss = 2.15 and accuracy of 0.44
Iteration 33024: with minibatch training loss = 2.4 and accuracy of 0.42
Iteration 33152: with minibatch training loss = 1.97 and accuracy of 0.46
Iteration 33280: with minibatch training loss = 2.43 and accuracy of 0.38
Iteration 33408: with minibatch training loss = 2.25 and accuracy of 0.43
Epoch 45, Overall loss = 2.21 and accuracy of 0.418
Iteration 33536: with minibatch training loss = 2.06 and accuracy of 0.41
Iteration 33664: with minibatch training loss = 2.1 and accuracy of 0.45
Iteration 33792: with minibatch training loss = 2.15 and accuracy of 0.44
Iteration 33920: with minibatch training loss = 2.11 and accuracy of 0.39
Iteration 34048: with minibatch training loss = 2.13 and accuracy of 0.41
Iteration 34176: with minibatch training loss = 2.07 and accuracy of 0.5
Epoch 46, Overall loss = 2.19 and accuracy of 0.423
Iteration 34304: with minibatch training loss = 2 and accuracy of 0.43
Iteration 34432: with minibatch training loss = 2.16 and accuracy of 0.46
Iteration 34560: with minibatch training loss = 2.36 and accuracy of 0.38
Iteration 34688: with minibatch training loss = 2.14 and accuracy of 0.43
Iteration 34816: with minibatch training loss = 2.29 and accuracy of 0.39
Epoch 47, Overall loss = 2.16 and accuracy of 0.429
Iteration 34944: with minibatch training loss = 2.14 and accuracy of 0.42
Iteration 35072: with minibatch training loss = 2.23 and accuracy of 0.41
Iteration 35200: with minibatch training loss = 2.37 and accuracy of 0.41
Iteration 35328: with minibatch training loss = 2.28 and accuracy of 0.44
Iteration 35456: with minibatch training loss = 2.05 and accuracy of 0.45
Iteration 35584: with minibatch training loss = 2.33 and accuracy of 0.41
Epoch 48, Overall loss = 2.15 and accuracy of 0.434
Iteration 35712: with minibatch training loss = 2.01 and accuracy of 0.42
Iteration 35840: with minibatch training loss = 2.14 and accuracy of 0.41
Iteration 35968: with minibatch training loss = 2.03 and accuracy of 0.48
Iteration 36096: with minibatch training loss = 2.22 and accuracy of 0.43
Iteration 36224: with minibatch training loss = 2.32 and accuracy of 0.38
Iteration 36352: with minibatch training loss = 2.03 and accuracy of 0.47
Epoch 49, Overall loss = 2.12 and accuracy of 0.437
Iteration 36480: with minibatch training loss = 2.07 and accuracy of 0.49
Iteration 36608: with minibatch training loss = 2.04 and accuracy of 0.48
Iteration 36736: with minibatch training loss = 2 and accuracy of 0.51
Iteration 36864: with minibatch training loss = 2.25 and accuracy of 0.38
Iteration 36992: with minibatch training loss = 1.96 and accuracy of 0.47
Iteration 37120: with minibatch training loss = 2.16 and accuracy of 0.46
Epoch 50, Overall loss = 2.09 and accuracy of 0.444
Iteration 37248: with minibatch training loss = 2.27 and accuracy of 0.4
Iteration 37376: with minibatch training loss = 1.97 and accuracy of 0.48
Iteration 37504: with minibatch training loss = 2.06 and accuracy of 0.42
Iteration 37632: with minibatch training loss = 2.27 and accuracy of 0.43
Iteration 37760: with minibatch training loss = 2.2 and accuracy of 0.45
Iteration 37888: with minibatch training loss = 1.84 and accuracy of 0.48
Epoch 51, Overall loss = 2.07 and accuracy of 0.45
Iteration 38016: with minibatch training loss = 1.86 and accuracy of 0.52
Iteration 38144: with minibatch training loss = 2.26 and accuracy of 0.44
Iteration 38272: with minibatch training loss = 1.95 and accuracy of 0.49
Iteration 38400: with minibatch training loss = 2.03 and accuracy of 0.51
Iteration 38528: with minibatch training loss = 2.02 and accuracy of 0.45
Epoch 52, Overall loss = 2.04 and accuracy of 0.455
Iteration 38656: with minibatch training loss = 2.2 and accuracy of 0.44
Iteration 38784: with minibatch training loss = 1.93 and accuracy of 0.5
Iteration 38912: with minibatch training loss = 2.22 and accuracy of 0.4
Iteration 39040: with minibatch training loss = 2 and accuracy of 0.51
Iteration 39168: with minibatch training loss = 1.99 and accuracy of 0.49
Iteration 39296: with minibatch training loss = 2.1 and accuracy of 0.45
Epoch 53, Overall loss = 2.02 and accuracy of 0.462
Iteration 39424: with minibatch training loss = 1.92 and accuracy of 0.51
Iteration 39552: with minibatch training loss = 1.94 and accuracy of 0.45
Iteration 39680: with minibatch training loss = 1.72 and accuracy of 0.53
Iteration 39808: with minibatch training loss = 1.92 and accuracy of 0.45
Iteration 39936: with minibatch training loss = 2.06 and accuracy of 0.44
Iteration 40064: with minibatch training loss = 2.01 and accuracy of 0.43
Epoch 54, Overall loss = 2 and accuracy of 0.464
Iteration 40192: with minibatch training loss = 1.94 and accuracy of 0.47
Iteration 40320: with minibatch training loss = 1.96 and accuracy of 0.42
Iteration 40448: with minibatch training loss = 2.07 and accuracy of 0.45
Iteration 40576: with minibatch training loss = 2.08 and accuracy of 0.43
Iteration 40704: with minibatch training loss = 1.91 and accuracy of 0.41
Iteration 40832: with minibatch training loss = 1.98 and accuracy of 0.5
Epoch 55, Overall loss = 1.98 and accuracy of 0.469
Iteration 40960: with minibatch training loss = 1.78 and accuracy of 0.47
Iteration 41088: with minibatch training loss = 2.02 and accuracy of 0.48
Iteration 41216: with minibatch training loss = 1.97 and accuracy of 0.52
Iteration 41344: with minibatch training loss = 1.97 and accuracy of 0.49
Iteration 41472: with minibatch training loss = 1.95 and accuracy of 0.47
Iteration 41600: with minibatch training loss = 1.67 and accuracy of 0.52
Epoch 56, Overall loss = 1.95 and accuracy of 0.477
Iteration 41728: with minibatch training loss = 1.97 and accuracy of 0.42
Iteration 41856: with minibatch training loss = 1.97 and accuracy of 0.5
Iteration 41984: with minibatch training loss = 1.83 and accuracy of 0.49
Iteration 42112: with minibatch training loss = 2.13 and accuracy of 0.41
Iteration 42240: with minibatch training loss = 1.81 and accuracy of 0.51
Epoch 57, Overall loss = 1.93 and accuracy of 0.477
Iteration 42368: with minibatch training loss = 1.99 and accuracy of 0.48
Iteration 42496: with minibatch training loss = 1.92 and accuracy of 0.49
Iteration 42624: with minibatch training loss = 1.84 and accuracy of 0.54
Iteration 42752: with minibatch training loss = 2.03 and accuracy of 0.41
Iteration 42880: with minibatch training loss = 1.68 and accuracy of 0.55
Iteration 43008: with minibatch training loss = 2.01 and accuracy of 0.45
Epoch 58, Overall loss = 1.91 and accuracy of 0.483
Iteration 43136: with minibatch training loss = 1.79 and accuracy of 0.54
Iteration 43264: with minibatch training loss = 2.11 and accuracy of 0.44
Iteration 43392: with minibatch training loss = 1.89 and accuracy of 0.51
Iteration 43520: with minibatch training loss = 1.98 and accuracy of 0.48
Iteration 43648: with minibatch training loss = 2.03 and accuracy of 0.47
Iteration 43776: with minibatch training loss = 2.27 and accuracy of 0.41
Epoch 59, Overall loss = 1.88 and accuracy of 0.49
Iteration 43904: with minibatch training loss = 1.84 and accuracy of 0.51
Iteration 44032: with minibatch training loss = 2.12 and accuracy of 0.41
Iteration 44160: with minibatch training loss = 1.91 and accuracy of 0.45
Iteration 44288: with minibatch training loss = 1.83 and accuracy of 0.53
Iteration 44416: with minibatch training loss = 1.87 and accuracy of 0.52
Iteration 44544: with minibatch training loss = 1.73 and accuracy of 0.5
Epoch 60, Overall loss = 1.87 and accuracy of 0.493
Iteration 44672: with minibatch training loss = 1.67 and accuracy of 0.5
Iteration 44800: with minibatch training loss = 1.7 and accuracy of 0.55
Iteration 44928: with minibatch training loss = 1.66 and accuracy of 0.59
Iteration 45056: with minibatch training loss = 2.03 and accuracy of 0.42
Iteration 45184: with minibatch training loss = 1.57 and accuracy of 0.57
Iteration 45312: with minibatch training loss = 1.97 and accuracy of 0.46
Epoch 61, Overall loss = 1.85 and accuracy of 0.498
Iteration 45440: with minibatch training loss = 1.88 and accuracy of 0.5
Iteration 45568: with minibatch training loss = 1.89 and accuracy of 0.52
Iteration 45696: with minibatch training loss = 1.69 and accuracy of 0.54
Iteration 45824: with minibatch training loss = 1.94 and accuracy of 0.43
Iteration 45952: with minibatch training loss = 1.68 and accuracy of 0.54
Epoch 62, Overall loss = 1.83 and accuracy of 0.503
Iteration 46080: with minibatch training loss = 2.01 and accuracy of 0.49
Iteration 46208: with minibatch training loss = 1.74 and accuracy of 0.52
Iteration 46336: with minibatch training loss = 1.82 and accuracy of 0.47
Iteration 46464: with minibatch training loss = 1.8 and accuracy of 0.5
Iteration 46592: with minibatch training loss = 1.88 and accuracy of 0.5
Iteration 46720: with minibatch training loss = 1.74 and accuracy of 0.5
Epoch 63, Overall loss = 1.8 and accuracy of 0.509
Iteration 46848: with minibatch training loss = 1.7 and accuracy of 0.55
Iteration 46976: with minibatch training loss = 1.79 and accuracy of 0.51
Iteration 47104: with minibatch training loss = 1.81 and accuracy of 0.48
Iteration 47232: with minibatch training loss = 1.91 and accuracy of 0.45
Iteration 47360: with minibatch training loss = 1.81 and accuracy of 0.52
Iteration 47488: with minibatch training loss = 1.53 and accuracy of 0.61
Epoch 64, Overall loss = 1.78 and accuracy of 0.514
Iteration 47616: with minibatch training loss = 1.56 and accuracy of 0.55
Iteration 47744: with minibatch training loss = 1.74 and accuracy of 0.52
Iteration 47872: with minibatch training loss = 1.89 and accuracy of 0.48
Iteration 48000: with minibatch training loss = 1.53 and accuracy of 0.6
Iteration 48128: with minibatch training loss = 1.74 and accuracy of 0.53
Iteration 48256: with minibatch training loss = 2.07 and accuracy of 0.42
Epoch 65, Overall loss = 1.76 and accuracy of 0.518
Iteration 48384: with minibatch training loss = 1.9 and accuracy of 0.48
Iteration 48512: with minibatch training loss = 1.63 and accuracy of 0.6
Iteration 48640: with minibatch training loss = 1.92 and accuracy of 0.44
Iteration 48768: with minibatch training loss = 1.76 and accuracy of 0.52
Iteration 48896: with minibatch training loss = 1.71 and accuracy of 0.55
Iteration 49024: with minibatch training loss = 1.87 and accuracy of 0.54
Epoch 66, Overall loss = 1.74 and accuracy of 0.522
Iteration 49152: with minibatch training loss = 1.67 and accuracy of 0.48
Iteration 49280: with minibatch training loss = 1.67 and accuracy of 0.52
Iteration 49408: with minibatch training loss = 1.82 and accuracy of 0.44
Iteration 49536: with minibatch training loss = 1.69 and accuracy of 0.55
Iteration 49664: with minibatch training loss = 1.74 and accuracy of 0.55
Epoch 67, Overall loss = 1.71 and accuracy of 0.528
Iteration 49792: with minibatch training loss = 2 and accuracy of 0.43
Iteration 49920: with minibatch training loss = 1.53 and accuracy of 0.55
Iteration 50048: with minibatch training loss = 1.56 and accuracy of 0.54
Iteration 50176: with minibatch training loss = 1.78 and accuracy of 0.48
Iteration 50304: with minibatch training loss = 1.8 and accuracy of 0.49
Iteration 50432: with minibatch training loss = 1.59 and accuracy of 0.55
Epoch 68, Overall loss = 1.7 and accuracy of 0.534
Iteration 50560: with minibatch training loss = 1.89 and accuracy of 0.48
Iteration 50688: with minibatch training loss = 1.73 and accuracy of 0.55
Iteration 50816: with minibatch training loss = 1.81 and accuracy of 0.51
Iteration 50944: with minibatch training loss = 1.55 and accuracy of 0.57
Iteration 51072: with minibatch training loss = 1.9 and accuracy of 0.52
Iteration 51200: with minibatch training loss = 1.62 and accuracy of 0.52
Epoch 69, Overall loss = 1.68 and accuracy of 0.536
Iteration 51328: with minibatch training loss = 1.64 and accuracy of 0.52
Iteration 51456: with minibatch training loss = 1.52 and accuracy of 0.61
Iteration 51584: with minibatch training loss = 1.7 and accuracy of 0.52
Iteration 51712: with minibatch training loss = 1.73 and accuracy of 0.57
Iteration 51840: with minibatch training loss = 1.72 and accuracy of 0.49
Iteration 51968: with minibatch training loss = 1.72 and accuracy of 0.55
Epoch 70, Overall loss = 1.66 and accuracy of 0.539
Iteration 52096: with minibatch training loss = 1.46 and accuracy of 0.55
Iteration 52224: with minibatch training loss = 1.51 and accuracy of 0.59
Iteration 52352: with minibatch training loss = 1.73 and accuracy of 0.49
Iteration 52480: with minibatch training loss = 1.52 and accuracy of 0.59
Iteration 52608: with minibatch training loss = 1.39 and accuracy of 0.58
Iteration 52736: with minibatch training loss = 1.76 and accuracy of 0.47
Epoch 71, Overall loss = 1.64 and accuracy of 0.544
Iteration 52864: with minibatch training loss = 1.81 and accuracy of 0.5
Iteration 52992: with minibatch training loss = 1.42 and accuracy of 0.57
Iteration 53120: with minibatch training loss = 1.56 and accuracy of 0.51
Iteration 53248: with minibatch training loss = 1.55 and accuracy of 0.58
Iteration 53376: with minibatch training loss = 1.7 and accuracy of 0.51
Epoch 72, Overall loss = 1.62 and accuracy of 0.551
Iteration 53504: with minibatch training loss = 1.58 and accuracy of 0.52
Iteration 53632: with minibatch training loss = 1.9 and accuracy of 0.47
Iteration 53760: with minibatch training loss = 1.77 and accuracy of 0.49
Iteration 53888: with minibatch training loss = 1.58 and accuracy of 0.55
Iteration 54016: with minibatch training loss = 1.66 and accuracy of 0.51
Iteration 54144: with minibatch training loss = 1.56 and accuracy of 0.57
Epoch 73, Overall loss = 1.6 and accuracy of 0.557
Iteration 54272: with minibatch training loss = 1.63 and accuracy of 0.55
Iteration 54400: with minibatch training loss = 1.45 and accuracy of 0.61
Iteration 54528: with minibatch training loss = 1.6 and accuracy of 0.51
Iteration 54656: with minibatch training loss = 1.6 and accuracy of 0.56
Iteration 54784: with minibatch training loss = 1.36 and accuracy of 0.64
Iteration 54912: with minibatch training loss = 1.34 and accuracy of 0.63
Epoch 74, Overall loss = 1.58 and accuracy of 0.559
Iteration 55040: with minibatch training loss = 1.45 and accuracy of 0.62
Iteration 55168: with minibatch training loss = 1.54 and accuracy of 0.55
Iteration 55296: with minibatch training loss = 1.46 and accuracy of 0.59
Iteration 55424: with minibatch training loss = 1.4 and accuracy of 0.58
Iteration 55552: with minibatch training loss = 1.74 and accuracy of 0.55
Iteration 55680: with minibatch training loss = 1.76 and accuracy of 0.55
Epoch 75, Overall loss = 1.57 and accuracy of 0.562
Iteration 55808: with minibatch training loss = 1.75 and accuracy of 0.52
Iteration 55936: with minibatch training loss = 1.49 and accuracy of 0.54
Iteration 56064: with minibatch training loss = 1.63 and accuracy of 0.55
Iteration 56192: with minibatch training loss = 1.51 and accuracy of 0.61
Iteration 56320: with minibatch training loss = 1.48 and accuracy of 0.52
Iteration 56448: with minibatch training loss = 1.67 and accuracy of 0.55
Epoch 76, Overall loss = 1.54 and accuracy of 0.569
Iteration 56576: with minibatch training loss = 1.52 and accuracy of 0.58
Iteration 56704: with minibatch training loss = 1.49 and accuracy of 0.6
Iteration 56832: with minibatch training loss = 1.65 and accuracy of 0.52
Iteration 56960: with minibatch training loss = 1.34 and accuracy of 0.62
Iteration 57088: with minibatch training loss = 1.48 and accuracy of 0.56
Epoch 77, Overall loss = 1.53 and accuracy of 0.573
Iteration 57216: with minibatch training loss = 1.43 and accuracy of 0.6
Iteration 57344: with minibatch training loss = 1.48 and accuracy of 0.57
Iteration 57472: with minibatch training loss = 1.64 and accuracy of 0.57
Iteration 57600: with minibatch training loss = 1.35 and accuracy of 0.63
Iteration 57728: with minibatch training loss = 1.56 and accuracy of 0.52
Iteration 57856: with minibatch training loss = 1.58 and accuracy of 0.53
Epoch 78, Overall loss = 1.51 and accuracy of 0.577
Iteration 57984: with minibatch training loss = 1.57 and accuracy of 0.59
Iteration 58112: with minibatch training loss = 1.54 and accuracy of 0.57
Iteration 58240: with minibatch training loss = 1.36 and accuracy of 0.62
Iteration 58368: with minibatch training loss = 1.61 and accuracy of 0.55
Iteration 58496: with minibatch training loss = 1.35 and accuracy of 0.6
Iteration 58624: with minibatch training loss = 1.49 and accuracy of 0.53
Epoch 79, Overall loss = 1.49 and accuracy of 0.581
Iteration 58752: with minibatch training loss = 1.43 and accuracy of 0.58
Iteration 58880: with minibatch training loss = 1.43 and accuracy of 0.59
Iteration 59008: with minibatch training loss = 1.54 and accuracy of 0.52
Iteration 59136: with minibatch training loss = 1.31 and accuracy of 0.6
Iteration 59264: with minibatch training loss = 1.47 and accuracy of 0.58
Iteration 59392: with minibatch training loss = 1.46 and accuracy of 0.59
Epoch 80, Overall loss = 1.47 and accuracy of 0.586
Iteration 59520: with minibatch training loss = 1.39 and accuracy of 0.54
Iteration 59648: with minibatch training loss = 1.42 and accuracy of 0.61
Iteration 59776: with minibatch training loss = 1.55 and accuracy of 0.56
Iteration 59904: with minibatch training loss = 1.26 and accuracy of 0.65
Iteration 60032: with minibatch training loss = 1.4 and accuracy of 0.59
Iteration 60160: with minibatch training loss = 1.46 and accuracy of 0.59
Epoch 81, Overall loss = 1.45 and accuracy of 0.588
Iteration 60288: with minibatch training loss = 1.45 and accuracy of 0.59
Iteration 60416: with minibatch training loss = 1.44 and accuracy of 0.62
Iteration 60544: with minibatch training loss = 1.21 and accuracy of 0.67
Iteration 60672: with minibatch training loss = 1.59 and accuracy of 0.59
Iteration 60800: with minibatch training loss = 1.3 and accuracy of 0.65
Epoch 82, Overall loss = 1.43 and accuracy of 0.595
Iteration 60928: with minibatch training loss = 1.33 and accuracy of 0.59
Iteration 61056: with minibatch training loss = 1.29 and accuracy of 0.65
Iteration 61184: with minibatch training loss = 1.54 and accuracy of 0.55
Iteration 61312: with minibatch training loss = 1.42 and accuracy of 0.58
Iteration 61440: with minibatch training loss = 1.38 and accuracy of 0.59
Iteration 61568: with minibatch training loss = 1.35 and accuracy of 0.61
Epoch 83, Overall loss = 1.42 and accuracy of 0.597
Iteration 61696: with minibatch training loss = 1.24 and accuracy of 0.62
Iteration 61824: with minibatch training loss = 1.26 and accuracy of 0.67
Iteration 61952: with minibatch training loss = 1.38 and accuracy of 0.62
Iteration 62080: with minibatch training loss = 1.47 and accuracy of 0.55
Iteration 62208: with minibatch training loss = 1.49 and accuracy of 0.59
Iteration 62336: with minibatch training loss = 1.35 and accuracy of 0.63
Epoch 84, Overall loss = 1.41 and accuracy of 0.601
Iteration 62464: with minibatch training loss = 1.33 and accuracy of 0.64
Iteration 62592: with minibatch training loss = 1.4 and accuracy of 0.59
Iteration 62720: with minibatch training loss = 1.3 and accuracy of 0.6
Iteration 62848: with minibatch training loss = 1.55 and accuracy of 0.53
Iteration 62976: with minibatch training loss = 1.29 and accuracy of 0.63
Iteration 63104: with minibatch training loss = 1.32 and accuracy of 0.55
Epoch 85, Overall loss = 1.38 and accuracy of 0.609
Iteration 63232: with minibatch training loss = 1.45 and accuracy of 0.59
Iteration 63360: with minibatch training loss = 1.29 and accuracy of 0.66
Iteration 63488: with minibatch training loss = 1.48 and accuracy of 0.59
Iteration 63616: with minibatch training loss = 1.81 and accuracy of 0.51
Iteration 63744: with minibatch training loss = 1.4 and accuracy of 0.59
Iteration 63872: with minibatch training loss = 1.24 and accuracy of 0.6
Epoch 86, Overall loss = 1.37 and accuracy of 0.611
Iteration 64000: with minibatch training loss = 1.14 and accuracy of 0.7
Iteration 64128: with minibatch training loss = 1.32 and accuracy of 0.66
Iteration 64256: with minibatch training loss = 1.45 and accuracy of 0.58
Iteration 64384: with minibatch training loss = 1.49 and accuracy of 0.61
Iteration 64512: with minibatch training loss = 1.11 and accuracy of 0.63
Iteration 64640: with minibatch training loss = 0.765 and accuracy of 0.83
Epoch 87, Overall loss = 1.35 and accuracy of 0.616
Iteration 64768: with minibatch training loss = 1.33 and accuracy of 0.6
Iteration 64896: with minibatch training loss = 1.45 and accuracy of 0.56
Iteration 65024: with minibatch training loss = 1.32 and accuracy of 0.62
Iteration 65152: with minibatch training loss = 1.47 and accuracy of 0.62
Iteration 65280: with minibatch training loss = 1.49 and accuracy of 0.56
Epoch 88, Overall loss = 1.33 and accuracy of 0.619
Iteration 65408: with minibatch training loss = 1.35 and accuracy of 0.59
Iteration 65536: with minibatch training loss = 1.27 and accuracy of 0.62
Iteration 65664: with minibatch training loss = 1.38 and accuracy of 0.62
Iteration 65792: with minibatch training loss = 1.41 and accuracy of 0.63
Iteration 65920: with minibatch training loss = 1.32 and accuracy of 0.61
Iteration 66048: with minibatch training loss = 1.28 and accuracy of 0.64
Epoch 89, Overall loss = 1.32 and accuracy of 0.621
Iteration 66176: with minibatch training loss = 1.23 and accuracy of 0.68
Iteration 66304: with minibatch training loss = 1.29 and accuracy of 0.66
Iteration 66432: with minibatch training loss = 1.28 and accuracy of 0.66
Iteration 66560: with minibatch training loss = 1.38 and accuracy of 0.63
Iteration 66688: with minibatch training loss = 1.08 and accuracy of 0.72
Iteration 66816: with minibatch training loss = 1.34 and accuracy of 0.59
Epoch 90, Overall loss = 1.3 and accuracy of 0.63
Iteration 66944: with minibatch training loss = 1.47 and accuracy of 0.55
Iteration 67072: with minibatch training loss = 1.3 and accuracy of 0.64
Iteration 67200: with minibatch training loss = 1.15 and accuracy of 0.66
Iteration 67328: with minibatch training loss = 1.19 and accuracy of 0.66
Iteration 67456: with minibatch training loss = 1.25 and accuracy of 0.65
Iteration 67584: with minibatch training loss = 1.4 and accuracy of 0.62
Epoch 91, Overall loss = 1.29 and accuracy of 0.628
Iteration 67712: with minibatch training loss = 1.41 and accuracy of 0.6
Iteration 67840: with minibatch training loss = 1.31 and accuracy of 0.63
Iteration 67968: with minibatch training loss = 1.18 and accuracy of 0.67
Iteration 68096: with minibatch training loss = 1.31 and accuracy of 0.63
Iteration 68224: with minibatch training loss = 1.28 and accuracy of 0.64
Iteration 68352: with minibatch training loss = 1.13 and accuracy of 0.68
Epoch 92, Overall loss = 1.27 and accuracy of 0.632
Iteration 68480: with minibatch training loss = 1.42 and accuracy of 0.63
Iteration 68608: with minibatch training loss = 1.33 and accuracy of 0.59
Iteration 68736: with minibatch training loss = 1.05 and accuracy of 0.73
Iteration 68864: with minibatch training loss = 1.43 and accuracy of 0.6
Iteration 68992: with minibatch training loss = 1.16 and accuracy of 0.64
Epoch 93, Overall loss = 1.26 and accuracy of 0.639
Iteration 69120: with minibatch training loss = 1.37 and accuracy of 0.6
Iteration 69248: with minibatch training loss = 1.19 and accuracy of 0.68
Iteration 69376: with minibatch training loss = 1.18 and accuracy of 0.66
Iteration 69504: with minibatch training loss = 1.03 and accuracy of 0.74
Iteration 69632: with minibatch training loss = 1.08 and accuracy of 0.73
Iteration 69760: with minibatch training loss = 1.46 and accuracy of 0.62
Epoch 94, Overall loss = 1.24 and accuracy of 0.642
Iteration 69888: with minibatch training loss = 1.14 and accuracy of 0.65
Iteration 70016: with minibatch training loss = 1.21 and accuracy of 0.68
Iteration 70144: with minibatch training loss = 1.34 and accuracy of 0.62
Iteration 70272: with minibatch training loss = 1.15 and accuracy of 0.7
Iteration 70400: with minibatch training loss = 1.33 and accuracy of 0.64
Iteration 70528: with minibatch training loss = 1.21 and accuracy of 0.68
Epoch 95, Overall loss = 1.23 and accuracy of 0.645
Iteration 70656: with minibatch training loss = 1.49 and accuracy of 0.6
Iteration 70784: with minibatch training loss = 1.23 and accuracy of 0.64
Iteration 70912: with minibatch training loss = 1.07 and accuracy of 0.69
Iteration 71040: with minibatch training loss = 1.17 and accuracy of 0.69
Iteration 71168: with minibatch training loss = 1.22 and accuracy of 0.66
Iteration 71296: with minibatch training loss = 1.12 and accuracy of 0.66
Epoch 96, Overall loss = 1.22 and accuracy of 0.648
Iteration 71424: with minibatch training loss = 1.18 and accuracy of 0.7
Iteration 71552: with minibatch training loss = 1.27 and accuracy of 0.62
Iteration 71680: with minibatch training loss = 1.29 and accuracy of 0.66
Iteration 71808: with minibatch training loss = 0.979 and accuracy of 0.73
Iteration 71936: with minibatch training loss = 1.01 and accuracy of 0.75
Iteration 72064: with minibatch training loss = 1.17 and accuracy of 0.62
Epoch 97, Overall loss = 1.2 and accuracy of 0.655
Iteration 72192: with minibatch training loss = 1.21 and accuracy of 0.64
Iteration 72320: with minibatch training loss = 1.21 and accuracy of 0.7
Iteration 72448: with minibatch training loss = 1.07 and accuracy of 0.71
Iteration 72576: with minibatch training loss = 1.28 and accuracy of 0.64
Iteration 72704: with minibatch training loss = 1.44 and accuracy of 0.6
Epoch 98, Overall loss = 1.19 and accuracy of 0.656
Iteration 72832: with minibatch training loss = 1.21 and accuracy of 0.68
Iteration 72960: with minibatch training loss = 1.02 and accuracy of 0.73
Iteration 73088: with minibatch training loss = 1.04 and accuracy of 0.69
Iteration 73216: with minibatch training loss = 1.31 and accuracy of 0.59
Iteration 73344: with minibatch training loss = 1.02 and accuracy of 0.76
Iteration 73472: with minibatch training loss = 1.17 and accuracy of 0.6
Epoch 99, Overall loss = 1.17 and accuracy of 0.662
Iteration 73600: with minibatch training loss = 1.12 and accuracy of 0.7
Iteration 73728: with minibatch training loss = 1.23 and accuracy of 0.61
Iteration 73856: with minibatch training loss = 1.13 and accuracy of 0.65
Iteration 73984: with minibatch training loss = 1.09 and accuracy of 0.63
Iteration 74112: with minibatch training loss = 1.15 and accuracy of 0.63
Iteration 74240: with minibatch training loss = 0.841 and accuracy of 0.75
Epoch 100, Overall loss = 1.16 and accuracy of 0.664
Validation
Epoch 1, Overall loss = 0.863 and accuracy of 0.747
Training
Epoch 1, Overall loss = 0.324 and accuracy of 0.931
Validation
Epoch 1, Overall loss = 0.863 and accuracy of 0.747
Test
Epoch 1, Overall loss = 0.444 and accuracy of 0.9
1 day, 0:29:33.916287